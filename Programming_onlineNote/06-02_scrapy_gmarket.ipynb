{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy  \n",
    "- 파이썬 언어를 이용한 웹 데이터 수집 프레임워크\n",
    "    - 프레임워크 vs 라이브러리 또는 패키지\n",
    "    - 프레임워크 : 특정목적을 가진 기능의 코드가 미리 설정되어서 빈칸채우기 식으로 코드를 작성\n",
    "    - 패키지는 다른 사람이 작성해 놓은 코드를 가져다가 사용하는 방법\n",
    " \n",
    "\n",
    "- scrapy\n",
    "    - `$pip install scrapy`\n",
    "- tree\n",
    "    - `$sudo apt install tree`  \n",
    "    \n",
    "서버에서 설치하기~~  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy는 자체적으로 강력한(?) xpath를 이용하기 때문에.\n",
    "Beautifulsoup을 굳이 쓸 필요없다.-> BeautifulSoup 쓰는 이유는 css-selector 쓰기 위해서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scrapy Project\n",
    "- scrapy 프로젝트 생성\n",
    "- scrapy 구조\n",
    "- gmarket 베스트 상품 링크 수집, 링크 안에 있는 상세 정보 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 프로젝트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: scrapy.cfg already exists in /home/ubuntu/02.Crawling/06.scrapy/crawler\r\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06-01_iterator_generator.ipynb\t06-03_xpath.ipynb  run.sh\r\n",
      "06-02_scrapy_gmarket.ipynb\tcrawler\t\t   scrapy.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mcrawler\u001b[00m\r\n",
      "├── \u001b[01;34mcrawler\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│   │   ├── __init__.cpython-36.pyc\r\n",
      "│   │   └── settings.cpython-36.pyc\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       ├── __init__.py\r\n",
      "│       └── \u001b[01;34m__pycache__\u001b[00m\r\n",
      "│           └── __init__.cpython-36.pyc\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "4 directories, 10 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### scrapy의 구조\n",
    "- spiders\n",
    "    - 어떤 웹서비스를 어떻게 크롤링할것인지에 대한 코드를 작성(.py 파일로 작성)\n",
    "- items.py\n",
    "    - 모델에 해당하는 코드, 저장하는 데이터의 자료구조를 설정\n",
    "- piplines.py\n",
    "    - 스크래핑한 결과물을 items 형태로 구성하고 처리하는 방법에 대한 코드\n",
    "- settings.py\n",
    "    - 스크래핑 할때의 설정값을 지정\n",
    "    - robots.txt : 따를지, 안따를지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gmarket 베스트 셀러 상품 수집\n",
    "- 상품명, 상세페이지 URL, 원가, 판매가, 할인율\n",
    "- xpath 확인\n",
    "- items.py\n",
    "- spider.py\n",
    "- 크롤러 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. xpath 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = requests.get(\"http://corners.gmarket.co.kr/Bestsellers\")\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items = response.xpath('//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li')\n",
    "len(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = response.xpath(\n",
    "    '//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li/div[1]/a/@href').extract()\n",
    "len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://item.gmarket.co.kr/Item?goodscode=1840147374&ver=637405265711220348'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-af4e23487aa2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"itemcase_basic\"]/h1/text()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0ms_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"itemcase_basic\"]/p/span/strong/text()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mo_price\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'//*[@id=\"itemcase_basic\"]/p/span/span/text()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdiscount_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_price\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"%\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo_price\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscount_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/parsel/selector.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, pos)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelectorList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## 결과데이터가 무조건 list라서 [0] 써준다.\n",
    "\n",
    "req = requests.get(links[0])\n",
    "response = TextResponse(req.url, body=req.text, encoding=\"utf-8\")\n",
    "title = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "s_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\n",
    "o_price = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "discount_rate = str(round((1 - int(s_price) / int(o_price))*100, 2)) + \"%\"\n",
    "title, s_price, o_price, discount_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. items.py 작성\n",
    "\n",
    "- 크롤링할 데이터의 모델을 정해줌."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import scrapy\r\n",
      "from crawler.items import CrawlerItem\r\n",
      "\r\n",
      "class Spider(scrapy.Spider):\r\n",
      "    name = \"GmarketBestsellers\"\r\n",
      "    allow_domain = [\"gmarket.co.kr\"]\r\n",
      "    start_urls = [\"http://corners.gmarket.co.kr/BestSellers\"]\r\n",
      "    \r\n",
      "    def parse(self, response):\r\n",
      "        links = response.xpath(\"//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li\")\r\n",
      "        for link in links[:10]:   ##실행하는데 시간이 너무많이 걸려서 links 10개까지만 하자.\r\n",
      "            yield scrapy.Request(link, callback=self.page_content)\r\n",
      "    \r\n",
      "    def page_content(self, response):\r\n",
      "        item = CrawlerItem()\r\n",
      "        \r\n",
      "        item[\"title\"] = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\r\n",
      "        item[\"s_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\r\n",
      "        try:\r\n",
      "            item[\"o_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\r\n",
      "        except:\r\n",
      "            item[\"o_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\r\n",
      "        item[\"discount_rate\"] = str(round((1 - int(s_price) / int(o_price))*100, 2)) + \"%\"\r\n",
      "        item[\"link\"] = response.url\r\n",
      "        yield item\r\n"
     ]
    }
   ],
   "source": [
    "!cat crawler/crawler/items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/items.py\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class CrawlerItem(scrapy.Item):\n",
    "    title = scrapy.Field()\n",
    "    s_price = scrapy.Field()\n",
    "    o_price = scrapy.Field()\n",
    "    discount_rate = scrapy.Field()\n",
    "    link = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spider.py 작성\n",
    "\n",
    "실제로 작성하는건 요정도밖에 안된다~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting crawler/crawler/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile crawler/crawler/items.py\n",
    "import scrapy\n",
    "from crawler.items import CrawlerItem\n",
    "\n",
    "class Spider(scrapy.Spider):\n",
    "    name = \"GmarketBestsellers\"\n",
    "    allow_domain = [\"gmarket.co.kr\"]\n",
    "    start_urls = [\"http://corners.gmarket.co.kr/BestSellers\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        links = response.xpath(\"//*[@id=\"gBestWrap\"]/div/div[3]/div[2]/ul/li\")\n",
    "        for link in links[:10]:   ##실행하는데 시간이 너무많이 걸려서 links 10개까지만 하자.\n",
    "            yield scrapy.Request(link, callback=self.page_content)\n",
    "    \n",
    "    def page_content(self, response):\n",
    "        item = CrawlerItem()\n",
    "        \n",
    "        item[\"title\"] = response.xpath('//*[@id=\"itemcase_basic\"]/h1/text()')[0].extract()\n",
    "        item[\"s_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/strong/text()')[0].extract().replace(\",\", \"\")\n",
    "        try:\n",
    "            item[\"o_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "        except:\n",
    "            item[\"o_price\"] = response.xpath('//*[@id=\"itemcase_basic\"]/p/span/span/text()')[0].extract().replace(\",\", \"\")\n",
    "        item[\"discount_rate\"] = str(round((1 - int(s_price) / int(o_price))*100, 2)) + \"%\"\n",
    "        item[\"link\"] = response.url\n",
    "        yield item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scrapy 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler  scrapy.cfg\r\n"
     ]
    }
   ],
   "source": [
    "!ls crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-08c2c89694a7>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-08c2c89694a7>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    cd crawler           # crawler 디렉토리로 이동\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## scrapy.cfg 파일이 있는 디렉토리 안에서 다음의 커맨드를 실행해야한다\n",
    "\n",
    "%%writefile run.sh\n",
    "cd crawler           # crawler 디렉토리로 이동\n",
    "scrapy crawl GmarketBestsellers     # shell script를 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# +x : 파일에 대한 실행권한을 추가해준다\n",
    "!chmod +x run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 14:01:05 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: crawler)\r\n",
      "2020-11-09 14:01:05 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Oct 19 2020, 01:13:33) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-5.4.0-1029-aws-x86_64-with-debian-buster-sid\r\n",
      "2020-11-09 14:01:05 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 75, in load\r\n",
      "    return self._spiders[spider_name]\r\n",
      "KeyError: 'GmarketBestsellers'\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/python3/bin/scrapy\", line 11, in <module>\r\n",
      "    sys.exit(execute())\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 145, in execute\r\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 100, in _run_print_help\r\n",
      "    func(*a, **kw)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 153, in _run_command\r\n",
      "    cmd.run(args, opts)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/commands/crawl.py\", line 22, in run\r\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 191, in crawl\r\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 224, in create_crawler\r\n",
      "    return self._create_crawler(crawler_or_spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 228, in _create_crawler\r\n",
      "    spidercls = self.spider_loader.load(spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 77, in load\r\n",
      "    raise KeyError(f\"Spider not found: {spider_name}\")\r\n",
      "KeyError: 'Spider not found: GmarketBestsellers'\r\n"
     ]
    }
   ],
   "source": [
    "!./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결과를 CSV로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile run.sh\n",
    "cd crawler\n",
    "scrapy crawl GmarketBestsellers -o GmarketBestsellers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-11-09 14:24:44 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: crawler)\r\n",
      "2020-11-09 14:24:44 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.6.9 (default, Oct 19 2020, 01:13:33) - [GCC 7.5.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1h  22 Sep 2020), cryptography 3.2.1, Platform Linux-5.4.0-1029-aws-x86_64-with-debian-buster-sid\r\n",
      "2020-11-09 14:24:44 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 75, in load\r\n",
      "    return self._spiders[spider_name]\r\n",
      "KeyError: 'GmarketBestsellers'\r\n",
      "\r\n",
      "During handling of the above exception, another exception occurred:\r\n",
      "\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/python3/bin/scrapy\", line 11, in <module>\r\n",
      "    sys.exit(execute())\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 145, in execute\r\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 100, in _run_print_help\r\n",
      "    func(*a, **kw)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/cmdline.py\", line 153, in _run_command\r\n",
      "    cmd.run(args, opts)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/commands/crawl.py\", line 22, in run\r\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 191, in crawl\r\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 224, in create_crawler\r\n",
      "    return self._create_crawler(crawler_or_spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/crawler.py\", line 228, in _create_crawler\r\n",
      "    spidercls = self.spider_loader.load(spidercls)\r\n",
      "  File \"/home/ubuntu/.pyenv/versions/3.6.9/envs/python3/lib/python3.6/site-packages/scrapy/spiderloader.py\", line 77, in load\r\n",
      "    raise KeyError(f\"Spider not found: {spider_name}\")\r\n",
      "KeyError: 'Spider not found: GmarketBestsellers'\r\n"
     ]
    }
   ],
   "source": [
    "! ./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crawler  scrapy.cfg\r\n"
     ]
    }
   ],
   "source": [
    "!ls crawler/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 정상적으로 크롤링되어 데이터가 저장되어 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = !pwd\n",
    "files = !ls crawler/\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls crawler/  # crawler프로젝트 안의 파일을 불러옴.\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"crawler/{}\".format(files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GmarketBestsellers.csv\")\n",
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Pipelines 설정\n",
    "- item 을 출력하기 전에 실행되는 코드를 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "class send_slack(object):\n",
    "    \n",
    "    def __send_slack(self, msg):\n",
    "        WEBHOOK_URL = \"\"\n",
    "        payload = {\n",
    "            \"channel\": \"\",\n",
    "            \"username\": \"\",\n",
    "            \"text\": msg,\n",
    "        }\n",
    "        requests.post(WEBHOOK_URL, json.dumps(payload))\n",
    "    \n",
    "    def process_item(self, item, spider):\n",
    "        keyword = \"세트\",\n",
    "        print(\"=\"*100)\n",
    "        print(item[\"title\"], keyword)\n",
    "        print(\"=\"*100)\n",
    "        if keyword in item[\"title\"]:\n",
    "            self.__send_slack(\"{},{},{}\".format(\n",
    "                item[\"title\"], item[\"s_price\"], item[\"link\"]))\n",
    "            \n",
    "        return item     #return된 item이 출력된당\n",
    "\n",
    "##실행해서 Overwriting 해준당"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pipeline 설정 : settings.py\n",
    "```\n",
    "ITEM_PIPELINES = {\n",
    "    'crawler.pipelines.CrawlerPipeline': 300,   \n",
    "}\n",
    "```\n",
    "\n",
    "- 숫자 : 1~1000까지 가능. 파이프라인의 순서 정해줌. 숫자 낮을수록 더 일찍 실행된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# echo : 뒤에오는 문자열을 출력시켜주는 shell 커맨드\n",
    "\n",
    "!echo \"ITEM_PIPELINES = {\" >> crawler/crawler/settings.py\n",
    "!echo \"    'crawler.pipelines.CrawlerPipeline': 300,\" >> crawler/crawler/settings.py\n",
    "!echo \"}\" >> crawler/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tail -n 3 cralwer/crawler/settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slack?? 으로 전송되는지 확인~~~\n",
    "\n",
    "!./run.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
